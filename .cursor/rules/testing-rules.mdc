---
description: Rules for testing best practices
globs:
alwaysApply: false
---
# VibeBiz Testing Best Practices - Updated Implementation
# Comprehensive testing standards for multi-tenant SaaS architecture
# Last Updated: 2025-01-01 - Hybrid Testing Framework Implementation

# =============================================================================
# ðŸš¨ CRITICAL TESTING REQUIREMENTS
# =============================================================================

## Mandatory Coverage Thresholds
- NEVER merge code without 80% minimum coverage (package-local tests)
- NEVER deploy without 90% minimum coverage (cross-cutting tests)
- ALWAYS test security functions with 100% coverage
- ALWAYS test multi-tenant isolation in integration tests
- NEVER skip testing authentication/authorization flows
- ALWAYS test API endpoints with 100% integration coverage
- NEVER deploy without E2E tests covering critical workflows

## Hybrid Testing Architecture
VibeBiz uses a **dual testing strategy** combining:
1. **Package-local tests** - Fast, isolated unit and component tests within each package
2. **Cross-cutting tests** - System-wide integration, security, performance, and E2E tests

## Test Types Required
- **Unit tests**: Fast, isolated, mock all dependencies (package-local)
- **Integration tests**: Real database, test service interactions (cross-cutting)
- **E2E tests**: Complete user workflows, cross-organizational scenarios (cross-cutting)
- **Security tests**: OWASP Top 10 validation, penetration testing (cross-cutting)
- **Performance tests**: Load testing, response time validation (cross-cutting)
- **Accessibility tests**: WCAG 2.2 AA compliance validation (cross-cutting)

# =============================================================================
# ðŸ—ï¸ TESTING ARCHITECTURE & STRUCTURE
# =============================================================================

## Package-Local Test Structure (Fast Feedback)
```
apps/public-web/
â”œâ”€â”€ __tests__/              # Component unit tests
â”œâ”€â”€ e2e/                   # App-specific E2E tests
â”œâ”€â”€ jest.config.ts         # Jest configuration
â”œâ”€â”€ jest.setup.js          # Test setup
â””â”€â”€ src/

services/public-api/
â”œâ”€â”€ tests/                 # API unit & integration tests
â”œâ”€â”€ conftest.py            # Pytest fixtures
â”œâ”€â”€ pyproject.toml         # Python test configuration
â””â”€â”€ src/

packages/types/
â”œâ”€â”€ __tests__/             # Type definition tests
â”œâ”€â”€ jest configuration     # In package.json
â””â”€â”€ src/
```

## Cross-Cutting Test Structure (System Validation)
```
tests/                     # Centralized cross-cutting tests
â”œâ”€â”€ integration/           # Multi-service integration tests
â”‚   â”œâ”€â”€ multi-tenant-isolation.test.js
â”‚   â”œâ”€â”€ auth-flow/
â”‚   â”œâ”€â”€ payment-flow/
â”‚   â””â”€â”€ tenant-isolation/
â”œâ”€â”€ security/              # Security vulnerability tests
â”‚   â”œâ”€â”€ owasp-top10.test.js
â”‚   â”œâ”€â”€ penetration/
â”‚   â”œâ”€â”€ vulnerability/
â”‚   â””â”€â”€ compliance/
â”œâ”€â”€ performance/           # Load and performance tests
â”‚   â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ stress/
â”‚   â””â”€â”€ benchmarks/
â”œâ”€â”€ e2e/                   # System-wide E2E tests
â”‚   â”œâ”€â”€ user-flows/
â”‚   â”œâ”€â”€ api/
â”‚   â””â”€â”€ mobile/
â”œâ”€â”€ accessibility/         # System-wide accessibility tests
â”œâ”€â”€ fixtures/              # Shared test data
â”‚   â””â”€â”€ shared-data.json
â”œâ”€â”€ config/                # Cross-cutting test configuration
â”‚   â””â”€â”€ jest.setup.js
â”œâ”€â”€ utils/                 # Test utilities and helpers
â”œâ”€â”€ jest.config.js         # Cross-cutting Jest configuration
â”œâ”€â”€ INTEGRATION_GUIDE.md   # Framework integration guide
â””â”€â”€ README.md              # Cross-cutting test documentation
```

## Root Test Configuration
```
jest.config.js             # Root Jest config (coordinates all projects)
jest.setup.js              # Global Jest setup
pytest.ini                 # Root pytest configuration
turbo.json                 # Turborepo test orchestration
```

# =============================================================================
# ðŸ”¬ PACKAGE-LOCAL TESTING STANDARDS
# =============================================================================

## Python Unit Testing (pytest)
- ALWAYS mock external dependencies (database, APIs, services)
- ALWAYS use factories for test data generation (Factory Boy)
- NEVER test implementation details, test behavior
- ALWAYS test both success and failure scenarios
- ALWAYS test edge cases and boundary conditions
- NEVER write tests that depend on external state
- ALWAYS use pytest markers: `@pytest.mark.unit`, `@pytest.mark.integration`

## TypeScript Unit Testing (Jest)
- ALWAYS test React components with React Testing Library
- ALWAYS mock API calls and external services
- NEVER test implementation details, test user interactions
- ALWAYS test component accessibility features
- ALWAYS test error states and loading states
- NEVER use shallow rendering, prefer full rendering
- ALWAYS use global test utilities via `testUtils`

## Test Structure Pattern (AAA Pattern)
```javascript
describe('Component Behavior', () => {
  it('should handle user interaction correctly', () => {
    // Arrange: Set up test data and mocks
    const mockHandler = jest.fn();
    const props = { onClick: mockHandler };

    // Act: Execute the function/component under test
    render(<Button {...props}>Click me</Button>);
    const button = screen.getByRole('button');
    fireEvent.click(button);

    // Assert: Verify expected behavior and side effects
    expect(mockHandler).toHaveBeenCalledTimes(1);
  });
});
```

# =============================================================================
# ðŸ”— CROSS-CUTTING INTEGRATION TESTING STANDARDS
# =============================================================================

## Multi-Tenant Integration Tests
- ALWAYS use real database for integration tests
- ALWAYS test with multiple organizations (multi-tenant isolation)
- NEVER test with shared test data between tests
- ALWAYS clean up test data after each test
- ALWAYS test database constraints and validations
- NEVER mock database layer in integration tests
- ALWAYS use `crossCuttingTestUtils.createMultiTenantData()`

## API Integration Tests
- ALWAYS test complete request/response cycles
- ALWAYS test authentication and authorization
- NEVER skip testing error responses (4xx, 5xx)
- ALWAYS test rate limiting and input validation
- ALWAYS test multi-tenant data isolation
- NEVER test with real external services (use test doubles)
- ALWAYS use `crossCuttingTestUtils.createApiTestClient()`

## Service Integration Tests
- ALWAYS test service-to-service communication
- ALWAYS test event handling and message queues
- NEVER test with production external services
- ALWAYS test timeout and retry mechanisms
- ALWAYS test circuit breaker functionality
- NEVER skip testing distributed transaction scenarios

## Integration Test Example
```javascript
describe('Multi-Tenant Data Isolation', () => {
  let apiClient, dbUtils, testData;

  beforeAll(async () => {
    apiClient = crossCuttingTestUtils.createApiTestClient();
    dbUtils = crossCuttingTestUtils.createDatabaseTestUtils();
    testData = crossCuttingTestUtils.createMultiTenantData();
    await dbUtils.seedTestData(testData);
  });

  it('should prevent cross-tenant data access', async () => {
    // Test implementation...
  });
});
```

# =============================================================================
# ðŸŒ END-TO-END TESTING STANDARDS
# =============================================================================

## E2E Test Requirements
- ALWAYS test complete user workflows
- ALWAYS test cross-organizational scenarios
- NEVER test individual components in isolation
- ALWAYS test with realistic data volumes
- ALWAYS test mobile and desktop viewports
- NEVER skip testing critical business processes

## E2E Tools and Patterns
- ALWAYS use Playwright for browser automation
- ALWAYS implement Page Object Model pattern
- NEVER hardcode test data in E2E tests
- ALWAYS test with multiple user roles
- ALWAYS test error scenarios and edge cases
- NEVER write flaky tests that randomly fail
- ALWAYS use shared fixtures from `tests/fixtures/shared-data.json`

## Critical E2E Workflows
- User registration and organization setup
- Authentication with MFA
- Organization switching and permissions
- Payment and subscription management
- Project creation and collaboration
- API key generation and usage

# =============================================================================
# ðŸ”’ SECURITY TESTING STANDARDS
# =============================================================================

## Security Test Implementation
- ALWAYS use `tests/security/owasp-top10.test.js` as baseline
- ALWAYS test against OWASP Top 10 vulnerabilities
- ALWAYS test authentication bypass scenarios
- NEVER skip testing authorization controls
- ALWAYS test input validation and sanitization
- ALWAYS test SQL injection prevention
- NEVER ignore security scanner findings
- ALWAYS use `crossCuttingTestUtils.createSecurityTestUtils()`

## Multi-Tenant Security Testing
- ALWAYS test cross-organization data access prevention
- ALWAYS test organization switching security
- NEVER skip testing data isolation at database level
- ALWAYS test API endpoint authorization
- ALWAYS test file upload security
- NEVER allow tests with hardcoded organization IDs

## Security Test Example
```javascript
describe('OWASP A03:2021 â€“ Injection', () => {
  it('should prevent SQL injection attacks', async () => {
    const securityUtils = crossCuttingTestUtils.createSecurityTestUtils();
    const results = await securityUtils.testSqlInjection(
      apiClient,
      '/api/v1/users/search',
      { query: 'test' }
    );

    results.forEach(result => {
      expect(result.blocked).toBe(true);
    });
  });
});
```

# =============================================================================
# âš¡ PERFORMANCE TESTING STANDARDS
# =============================================================================

## Performance Test Requirements
- ALWAYS test API response times under load
- ALWAYS test database query performance
- NEVER ignore memory leaks in long-running tests
- ALWAYS test with realistic data volumes
- ALWAYS test concurrent user scenarios
- NEVER skip testing resource utilization
- ALWAYS use `crossCuttingTestUtils.createPerformanceTestUtils()`

## Load Testing Targets (from shared-data.json benchmarks)
- API endpoints: < 200ms response time under normal load
- Database queries: < 100ms for simple queries
- Page loads: < 3s for initial load, < 1s for subsequent
- File uploads: Support files up to 100MB
- Concurrent users: 1000+ simultaneous users
- Data processing: Handle 10k+ records per operation

## Performance Test Example
```javascript
it('should maintain acceptable response times', async () => {
  const performanceUtils = crossCuttingTestUtils.createPerformanceTestUtils();

  const result = await performanceUtils.measureResponseTime(
    async () => await apiClient.get('/api/v1/documents'),
    5 // iterations
  );

  performanceUtils.assertResponseTime(result.average, 200, 'Document list API');
});
```

# =============================================================================
# â™¿ ACCESSIBILITY TESTING STANDARDS
# =============================================================================

## Accessibility Requirements
- ALWAYS test with screen readers (NVDA, JAWS, VoiceOver)
- ALWAYS validate keyboard navigation
- NEVER ignore color contrast violations
- ALWAYS test with assistive technologies
- ALWAYS validate ARIA labels and roles
- NEVER skip testing focus management
- ALWAYS use `@axe-core/playwright` for automated testing

## WCAG 2.2 AA Compliance
- Color contrast ratio: 4.5:1 for normal text, 3:1 for large text
- Keyboard navigation: All functionality accessible via keyboard
- Screen reader compatibility: Proper semantic markup
- Focus indicators: Visible focus indicators for all interactive elements
- Alternative text: Meaningful alt text for all images
- Form labels: Proper labels for all form controls

# =============================================================================
# ðŸ› ï¸ TEST TOOLING AND INFRASTRUCTURE
# =============================================================================

## Package-Local Testing Tools
- **Python**: pytest, pytest-asyncio, pytest-cov, factory-boy
- **TypeScript**: Jest, React Testing Library, ts-jest
- **Mocking**: MSW (Mock Service Worker), jest.fn()
- **E2E**: Playwright with TypeScript (app-specific)

## Cross-Cutting Testing Tools
- **Integration**: Jest with custom configuration (`tests/jest.config.js`)
- **Security**: OWASP ZAP, Bandit, Semgrep, custom security utils
- **Performance**: K6 or Artillery, custom performance utils
- **E2E**: Playwright (system-wide scenarios)
- **Accessibility**: axe-core, Pa11y

## Test Utilities Available
```javascript
// Package-local utilities (jest.setup.js)
global.testUtils = {
  createMockUser: (overrides) => ({...}),
  createMockOrganization: (overrides) => ({...}),
  mockApiResponse: (data, status) => ({...}),
  waitFor: (ms) => Promise
};

// Cross-cutting utilities (tests/config/jest.setup.js)
global.crossCuttingTestUtils = {
  createMultiTenantData: () => ({...}),
  createApiTestClient: (baseURL) => ({...}),
  createDatabaseTestUtils: () => ({...}),
  createPerformanceTestUtils: () => ({...}),
  createSecurityTestUtils: () => ({...}),
  waitForCondition: (fn, timeout, interval) => Promise,
  retryAsync: (fn, maxRetries, delay) => Promise
};
```

## Test Environment Management
- ALWAYS use isolated test databases
- ALWAYS clean up test data between tests
- NEVER share test environments between developers
- ALWAYS use Docker for consistent test environments
- ALWAYS seed test data with factories or shared fixtures
- NEVER use production data in tests
- ALWAYS use separate database URLs for cross-cutting tests

# =============================================================================
# ðŸ”„ DEVELOPER WORKFLOW INTEGRATION
# =============================================================================

## Daily Development Workflow
```bash
# Fast feedback loop - package-local tests
cd apps/public-web
pnpm test:watch

# Quick validation
pnpm test                    # All package-local tests
pnpm test:unit              # Only unit tests
```

## Pre-Commit Workflow
```bash
# Comprehensive validation
pnpm test                    # Package-local tests
pnpm test:cross-cutting     # Cross-cutting tests
pnpm test:security          # Security tests
```

## Pre-Deployment Workflow
```bash
# Full system validation
pnpm test:all               # Everything
pnpm test:performance       # Performance validation
pnpm test:accessibility     # Accessibility compliance
```

## Available Test Commands
```json
{
  "scripts": {
    "test": "turbo run test",                    // Package-local tests
    "test:unit": "turbo run test:unit",          // Unit tests only
    "test:integration": "turbo run test:integration", // Package integration
    "test:e2e": "turbo run test:e2e",            // Package E2E tests
    "test:cross-cutting": "cd packages/types && npx jest --config ../../tests/jest.config.js",
    "test:performance": "cd packages/types && npx jest --config ../../tests/jest.config.js --testPathPattern=performance",
    "test:security": "cd packages/types && npx jest --config ../../tests/jest.config.js --testPathPattern=security",
    "test:all": "npm run test && npm run test:cross-cutting"
  }
}
```

# =============================================================================
# ðŸ“Š TEST DATA MANAGEMENT
# =============================================================================

## Shared Test Data (tests/fixtures/shared-data.json)
- **Organizations**: Multi-tenant test organizations with realistic settings
- **Users**: Admin and regular users across different organizations
- **Documents**: Sample documents with proper tenant isolation
- **API Responses**: Standard response templates for mocking
- **Performance Benchmarks**: SLA targets and performance thresholds
- **Security Test Data**: Malicious payloads for security testing

## Test Data Principles
- ALWAYS use factories for generating test data
- NEVER hardcode test data values
- ALWAYS create realistic test scenarios
- NEVER use production data in tests
- ALWAYS clean up test data after tests
- NEVER share test data between test cases
- ALWAYS use shared fixtures for cross-cutting tests

## Multi-Tenant Test Data
- ALWAYS create separate organizations for each test
- NEVER test with shared organizational data
- ALWAYS validate data isolation between tenants
- NEVER skip testing cross-tenant access prevention
- ALWAYS test with multiple user roles per organization
- NEVER use the same user across multiple organizations

# =============================================================================
# ðŸ“‹ QUALITY GATES & CI/CD INTEGRATION
# =============================================================================

## Required Before Merge
- âœ… All package-local tests pass (80% coverage minimum)
- âœ… Cross-cutting integration tests pass (90% coverage minimum)
- âœ… Security tests pass (100% coverage on auth/authz)
- âœ… Performance tests within acceptable limits
- âœ… Accessibility tests pass (WCAG 2.2 AA)
- âœ… No flaky tests introduced
- âœ… Test documentation updated

## CI/CD Pipeline Integration
```yaml
# Package-local tests (fast feedback)
unit-tests:
  run: pnpm test

# Cross-cutting tests (comprehensive validation)
integration-tests:
  needs: unit-tests
  run: |
    pnpm test:cross-cutting
    pnpm test:performance
    pnpm test:security
```

## Continuous Monitoring
- âœ… Test execution time trending
- âœ… Test failure rate monitoring
- âœ… Coverage trend analysis (package-local vs cross-cutting)
- âœ… Security scan results
- âœ… Performance regression detection
- âœ… Accessibility compliance monitoring

# =============================================================================
# ðŸš« TESTING ANTI-PATTERNS TO AVOID
# =============================================================================

## Never Do These
- NEVER write tests that depend on execution order
- NEVER use sleep() or arbitrary timeouts in tests
- NEVER test private methods directly
- NEVER write tests that require manual setup
- NEVER ignore intermittently failing tests
- NEVER write tests without clear assertions
- NEVER test multiple concerns in a single test
- NEVER commit tests that don't run locally
- NEVER write tests that take longer than necessary
- NEVER skip writing tests because "it's simple code"
- NEVER mix package-local and cross-cutting test concerns
- NEVER hardcode organization IDs in multi-tenant tests

# =============================================================================
# ðŸ“š DOCUMENTATION AND RESOURCES
# =============================================================================

## Required Documentation
- **tests/README.md**: Cross-cutting test documentation
- **tests/INTEGRATION_GUIDE.md**: Framework integration guide
- **Package READMEs**: Individual package testing documentation
- **Test utilities**: Inline documentation for all test helpers

## Getting Started
1. Read `tests/INTEGRATION_GUIDE.md` for framework overview
2. Review example tests in `tests/integration/` and `tests/security/`
3. Use shared fixtures from `tests/fixtures/shared-data.json`
4. Follow test utilities patterns in `tests/config/jest.setup.js`

## Resources
- [Jest Configuration](mdc:jest.config.js) - Root Jest coordination
- [Cross-cutting Jest Config](mdc:tests/jest.config.js) - System-wide test config
- [Pytest Configuration](mdc:pytest.ini) - Python testing standards
- [Shared Test Data](mdc:tests/fixtures/shared-data.json) - Fixtures and benchmarks

Remember: Testing is not optional. Quality cannot be retrofitted. Our hybrid approach ensures both fast development feedback and comprehensive system validation. Test early, test often, and test comprehensively.
